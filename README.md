# DSL_Final_Project
# Implementing End-to-End Object Detection Model With Hardware Acceleration On The Beaglebone AI-64
By Paul Han, Drew Conyers, Michael Routh, Tristan Becnel, Kenneth Pinzon, Isabel Aguilera, Tanya Shiramagond, and Anthony Lam

![https://cdn.discordapp.com/attachments/1095549530583343116/1100186102838607982/Fleet_Render_1x1.png](https://cdn.discordapp.com/attachments/1095549530583343116/1100186102838607982/Fleet_Render_1x1.png)

## Section 1: Problem Overview
The Robomaster competition, an annual robotics contest held by DJI, gathers teams from universities and institutions worldwide to participate in a series of challenges using robotics and computer vision. The main competition at this event is the robot battle, where teams design and build custom robots to compete in an arena style game. Within the competition environment robots navigate through the course, identify and interact with specific objects, and engage in simulated combat with opposing teams’ robots. The game comprises multiple rounds, with each team’s objective being to score points by accurately hitting their opponent’s robots’ armor plates. The armor plates are equipped with lights and numbers, and damage is only registered when these plates are hit. This unique aspect of the competition emphasizes the importance of computer vision and object detection capabilities, as robots must quickly and accurately identify and target the armor plates of their adversaries to score points and ultimately achieve victory. 

![https://media.discordapp.net/attachments/1095549530583343116/1100193804864213114/image.png?width=1031&height=662](https://media.discordapp.net/attachments/1095549530583343116/1100193804864213114/image.png?width=1031&height=662)

Our goal for the project was to develop an end-to-end object detection model using YOLO and the Beaglebone AI-64. As detection and aim at armor plates is significantly faster when done by a program rather than manual human aim through a mouse and keyboard, an end-to-end object detection model is crucial for better performance in the Robomaster competition. In the previous semester (Fall 2022), a Data Science Lab team also developed an end-to-end object detection model using YOLO v5 and deployed the model on an NVIDIA Jetson Nano. However, the UT Robomaster team, Stampede, uses only the Beaglebone AI-64 SoC for each of their three robots. As the Jetson Nano SoC and the Beaglebone AI-64 are significantly different (ARM Cortex-A72 on Beaglebone vs. ARM Cortex-A57 processor on Jetson Nano along with significantly different hardware and Debian images), development and deployment of a YOLO model on the Beaglebone is significantly more challenging due to lack of documentation for Beaglebone and lack of significant number of users for the Beaglebone as well. 

In this project, we used an object detection machine learning model called YOLO v5 (YOLO for You Only Look Once) in order to detect Robomaster armor plates with high, consistent accuracy. We chose YOLO v5 because TI, the developer of the chip for the Beaglebone, has a custom build of YOLO v5 that allows easier access to the hardware acceleration built into the TI chip's architecture. Their product is called the TI Deep Learning Product or TIDL for short. This proprietary build of YOLO v5 utilizes both an ONNX file and a prototxt file. The usage of this ONNX file allows easy access to optimizations across multiple architectures and allows YOLO to have significantly faster inference for detection compared to other models. Additionally, YOLO v5 has significantly higher accuracy and speed for detection compared to other models such as YOLO v3, making it a better choice for our project. Additionally, the Beaglebone was chosen as the SoC as it is the primary board for the UT Robomaster CV team. 

In order to develop YOLO, we first had to train YOLO on a custom dataset. This was done by manually labeling a train dataset of images of robots that have an armor plate of both red and blue colors, supplied from Purdue's Robomaster team, Boilerbots, and the UT Robomaster team as well. 3,895 images were labeled using CVAT or Computer Vision Annotation Tool and then trained on Google Colab. A Jupyter notebook supplied by a University of Washington graduate, Kaelin Laundry, who was also on the UW Robomaster team, ARUW, was used to train and export the model. Once the model was trained, we exported the model's ONNX files and prototxt files to the Beaglebone through SSH and utilized the onnxruntime Python library to access the optimizations available on the Beaglebone and ran the Beaglebone on images of Robomaster bots from Purdue. 

The overall pipeline for the model can be seen as a graphic below.
![https://cdn.discordapp.com/attachments/1095549530583343116/1100199334504562698/Software.png](https://cdn.discordapp.com/attachments/1095549530583343116/1100199334504562698/Software.png)

## Section 2: YOLO v5 Overview
YOLO v5 is a computer vision model and is used for object detection. It performs well relative to its inference time, so it is known to process images quickly with a high degree of accuracy.
The YOLO v5 network is made of 3 pieces: the backbone, the neck, and the head. The backbone is a convolutional neural network that takes the image input and forms image features by putting it through different layers. The neck merges these image features together to make a prediction. The head forms a class prediction based on the acquired features, which overall creates a sparse prediction of the location of the detected object.
YOLO v5 uses data augmentation and loss calculation techniques as their main training procedures. In our case, the YOLO model took the Robomaster images with labeled armor plates and used these training procedures to learn how to detect the armor plates. 

## Section 3: Data Collection and Preprocessing
Our dataset of images is from the Boilerplates Purdue University Robomaster team and from the UT Robomaster team Stampede. The images that were provided consisted of metal plates that had blue lights and of metal plates that had red lights. We then proceeded to label these images using a publicly available tool called CVAT (Computer Vision Annotation Tool, cvat.ai). We created two distinct labels to distinguish these plates with a red label titled “Red Plate” for plates with red lights and a blue label titled “Blue Plate” for plates with blue lights. Each team member was responsible for labeling ~500 images. Using CVAT, we each labeled 500 plates by drawing squares around the metal plates. If the image was too blurry or both lights on the plate were not visible then we would skip that image. Once we were finished labeling the images we exported the data as YOLO 1.1 format. Once every team member was finished we were now ready to start training and optimizing our model.
![https://media.discordapp.net/attachments/1095549530583343116/1100194213934673990/image.png?width=1375&height=662](https://media.discordapp.net/attachments/1095549530583343116/1100194213934673990/image.png?width=1375&height=662)

## Section 4: Performance Optimization with ONNX
ONNX or Open Neural Network Exchange is an open-source library dedicated to facilitating the exchange of neural networks between different frameworks and tools. The library allows for the expression of different deep learning models to be represented in a common format which allows models to be used across platforms and multiple frameworks. 

Our team is capitalizing on ONNX’s efficiency and model optimization to maximize our YOLO's performance. ONNX supports hardware acceleration on specialized accelerators which boosts performance due to the standardization of model formatting. ONNX also supports quantization, reducing memory usage and computation time as well as having the ability of ‘pruning’ which gets rid of unnecessary model complexity.

Lastly, although there may be other machine learning model optimization libraries like NVIDIA’s CUDA, our model was made by Texas Instruments and our beaglebone has proprietary TI chips integrated within the SoC. Therefore the decision to use ONNX is obvious. The library is the most compatible with the model, hardware, and can push the most performance out of our project. Additionally, TI has optimizations available for specifically YOLO v5 which aided our selection of YOLO v5 and the Beaglebone.

## Section 5: Integrating with Beaglebone AI-64 and Intel Realsense Camera
Integrating the YOLO v5 object detection model with the Beaglebone AI-64 SoC and the Intel Realsense camera involves the following series of steps to optimize the detection of armor plates in the Robomaster competition:

We first connect the Beaglebone to a laptop in order to SSH into the Beaglebone. UT's IoT wifi would not allow the registration of the Beaglebone so in order to work on the Beaglebone, we used a USB-C to USB-C cable to connect to the Beaglebone.

The second step is to initialize and configure the BeagleBone. Booting of Beaglebone is done through an SD card with the latest Debian image available. The boot is done with the image on the the eMMC (Embedded MultiMediaCard) of the board. While it is prudent to boot off of the SD card, the SD card boot was facing issues so we booted off the eMMC.

The third step is to install all required firmware. The major package to be installed is [BeagleBoard Device Trees](https://github.com/beagleboard/BeagleBoard-DeviceTrees/). Device Trees for Debian images is Linux's method of having a standardized method of configuring hardware such as PINMUX, UART pins, etc. This allows us to configure where data is input, output, and is crucial to the performance of the board. Installation of Python and libraries required for running the YOLO v5 model, such as PyTorch and OpenCV. Additionally, drivers and software for Intel Realsense cameras must be installed to enable communication between the camera and the Beaglebone AI-64 board. The Beaglebone AI-64 board is properly configured with all software dependencies installed. 

The fourth step is to enable internet access on the BeagleBone. The default gateway is set to the local connection (i.e laptop's) default IP address, 192.168.7.1. The time date is then configured to the Chicago timezone, Network Time Protocol is disabled, and is set to military time. Set /etc/resolv.conf as open by adding  “nameserver 8.8.8.8” to the end of the file. The final step is to allow traffic from the Beaglebone to route through Windows which can be found here ->, [detailed explanation can be found here](https://www.digikey.com/en/maker/blogs/how-to-connect-a-beaglebone-black-to-the-internet-using-usb).

The fifth and final step is to install the Python Realsense wrapper on the Beaglebone. Detailed instructions for the package installation are [found here](https://github.com/IntelRealSense/librealsense/tree/master/wrappers/python).

![https://media.discordapp.net/attachments/1095549530583343116/1100207977060909106/Screen_Shot_2023-04-24_at_6.53.36_PM.png?width=1440&height=551](https://media.discordapp.net/attachments/1095549530583343116/1100207977060909106/Screen_Shot_2023-04-24_at_6.53.36_PM.png?width=1440&height=551)

## Section 6: Test and Evaluation
We tested our trained data model on the images provided by the UT Robomaster Stampede team and at first our model was insufficient with improperly labeling plates with red lights. From our initial testing, we lowered the learning rate by a factor of 15 times and this improved our data model. We also lowered the resolution because smaller images have faster inference times. With faster inference times and a significantly smaller learning rate we were able to improve upon our data model. In the images you can see how the metal plates are properly labeled and the image with a lack of plate has no labeling. There are some false positives when the image is too blurry and there is an overwhelming presence of blue or red lights shown then the model will give it a false label. Overall, the data model performs excellent.
![https://media.discordapp.net/attachments/1095549530583343116/1100197031458377758/image.png?width=932&height=528](https://media.discordapp.net/attachments/1095549530583343116/1100197031458377758/image.png?width=932&height=528)

![https://media.discordapp.net/attachments/1095549530583343116/1100197186312089640/image.png?width=901&height=503](https://media.discordapp.net/attachments/1095549530583343116/1100197186312089640/image.png?width=901&height=503)

## Conclusion
In conclusion, we used a YOLO v5 model to successfully detect Robomaster armor plates on a Beaglebone AI-64. Our process consisted of labeling armor plates, using Open Neural Network Exchange to maximize the performance of the model, and training it on the Beaglebone. Aside from time constraints, the biggest difficulty was learning how to use YOLO and integrate it with the live feed coming from the Realsense camera. Another difficulty is the process of labeling images on CVAT. We believe we could improve upon our current YOLO v5 model by labeling more images that had more diverse backgrounds (i.e, not the same Purdue basement, but arenas, different colored robots, etc.) but we were only given about 4000 images from the same background to work with from Purdue. In the end the latency for detection on YOLO was 7.56 ms which was right around the 7.5ms benchmark time. This was significantly better than YOLO without optimizations as inferencing on an AMD Ryzen 9 4900HS, one of the most powerful CPUs on the market for laptops, inferenced detections at approximately 35 ms, which means YOLO on the Beaglebone ran approximately 5x faster. So far, no one has been able to successfully run YOLO and the Realsense on the Beaglebone in operation so in this, our approach has been one of territory unexplored.

# Improvements
Sadly, as the power supply currently on the Beaglebone was insufficient to run YOLO and the Realsense which consumes roughly 2W with maximum power supplied to the Beaglebone being 10W right now, we were not able to get YOLO working on the live feed from the Realsense. However, the live feed from the Realsense was able to be exported through X11 Forwarding and displayed on Paul's laptop. YOLO was also able to inference detections on sample images that were withheld from the training dataset in approximately 7.5ms. With a sufficient power supply, simultaneous operation would have occurred and would be the next step in development for performance. Additionally, we could train YOLO with more diverse data such as different backgrounds, different color robots in order to improve performance. Since different model robots such as the Standard, Sentry, Hero, Aerial, Engineer, are distinguished on armor plates through different numbers, we could train YOLO on labeled images with different numbers in order to distinguish between different robots which would allow priorization of targets easily.

## Sources
The GitHub repo for YOLO v5 -  [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) 

The Beaglebone AI-64 - [https://beagleboard.org/ai-64](https://beagleboard.org/ai-64) 

About YOLOv5 - [https://blog.roboflow.com/yolov5-improvements-and-evaluation/](https://blog.roboflow.com/yolov5-improvements-and-evaluation/) 

A guide for using YOLOv5 on the Beaglebone AI-64 - [https://github.com/WasabiFan/tidl-yolov5-custom-model-demo](https://github.com/WasabiFan/tidl-yolov5-custom-model-demo)

Paul created two guides, one for [labeling images using CVAT](https://docs.google.com/document/d/1Dv-AmiVg_wX1D8T-aZfzkl14iyU11Dd2TfjmNSOxmIE/edit) and another for [setting upx the Beaglebone](https://docs.google.com/document/d/1eZiniOKkxNeE0nyb9P3TmpwKyLvC_PayqkE7lATLaPI/edit#heading=h.kjws9f15q4ae) 
